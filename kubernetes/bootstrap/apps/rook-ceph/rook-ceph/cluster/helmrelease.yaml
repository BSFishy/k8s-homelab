---
# yaml-language-server: $schema=https://k8s-schemas.bjw-s.dev/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: rook-ceph-cluster
spec:
  interval: 10m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: v1.18.1
  url: oci://ghcr.io/rook/rook-ceph-cluster
---
# yaml-language-server: $schema=https://k8s-schemas.bjw-s.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
    - name: traefik
      namespace: infra
    - name: snapshot-controller
      namespace: system-controllers
  values:
    monitoring:
      enabled: true
      createPrometheusRules: true

    toolbox:
      enabled: true

    cephClusterSpec:
      cephConfig:
        global:
          bdev_enable_discard: "true" # quote
          bdev_async_discard_threads: "1" # quote
          osd_class_update_on_start: "false" # quote
          device_failure_prediction_mode: local # requires mgr module
        mgr:
          mgr/crash/warn_recent_interval: "7200" # 2h
        mds:
          mds_cache_memory_limit: "4294967296" # 4GiB cache for large media trees

      cleanupPolicy:
        wipeDevicesFromOtherClusters: false

      crashCollector:
        disable: false

      dashboard:
        enabled: true
        urlPrefix: /
        ssl: true
        prometheusEndpoint: http://mimir-ceph-proxy.observability.svc.cluster.local:8080
        alertmanagerEndpoint: http://mimir-ceph-proxy.observability.svc.cluster.local:8081/alertmanager

      mgr:
        modules:
          - name: diskprediction_local
            enabled: true
          - name: insights
            enabled: true
          - name: pg_autoscaler
            enabled: true
          - name: rook
            enabled: true

      network:
        provider: host
        connections:
          requireMsgr2: true

      storage:
        useAllNodes: false
        useAllDevices: false
        devicePathFilter: "^/dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_"
        devices:
          # gaia-01
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y716078M

          # gaia-02
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y708885P

          # gaia-03
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y707633R

          # gaia-04
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0YA28274E

          # gaia-05
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0YA32820Z

        config:
          osdsPerDevice: "1"

    cephBlockPools:
      # Critical: size=3, retain, snapshot/backup targeted by Velero
      - name: pool-critical
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-critical
          isDefault: false
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # label SC so Velero schedules can target it
          annotations:
            storage-tier: "critical"

      # High priority: size=3, normal reclaim
      - name: pool-high
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-high
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "high"

      # Regular: size=2 or 3 (pick), baseline
      - name: pool-regular
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-regular
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "regular"

      # Low: size=2, cheaper
      - name: pool-low
        spec:
          failureDomain: host
          replicated:
            size: 2
            min_size: 1
            requireSafeReplicaSize: false
        storageClass:
          enabled: true
          name: ceph-low
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "low"

      # Extra-low: “no snapshots/backups” (policy-level), minimal features
      - name: pool-extra-low
        spec:
          failureDomain: host
          replicated:
            size: 2
            min_size: 1
            requireSafeReplicaSize: false
        storageClass:
          enabled: true
          name: ceph-extra-low
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "extra-low"
            # Optional: an annotation your Velero schedule uses to EXCLUDE this class
            backup.velero.io/exclude: "true"

    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete
      labels:
        velero.io/csi-volumesnapshot-class: "true"

    cephFileSystems:
      - name: ceph-media
        spec:
          # pools
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
          dataPools:
            - name: cephfs-data
              failureDomain: host
              replicated:
                size: 2
                min_size: 1
                requireSafeReplicaSize: false
              parameters:
                compression_mode: none
          preserveFilesystemOnDelete: false # allow rebuild of filesystem/pools when needed
          # MDS HA so RWX is reliable
          metadataServer:
            activeCount: 2
            activeStandby: true
            resources:
              requests:
                cpu: 500m
                memory: 2Gi
              limits:
                memory: 4Gi
        storageClass:
          enabled: true
          name: ceph-media
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions:
            - rw
            - noatime
            - readdir_max_entries=1024
            - client_mount_timeout=120
            - client_oc_size=524288
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            fsName: ceph-media
            pool: ceph-media-cephfs-data
            mounter: kernel

    cephObjectStores:
      - name: ceph-objectstore
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
          dataPool:
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          preservePoolsOnDelete: true
          gateway:
            port: 80
            resources:
              requests:
                cpu: 1000m
                memory: 1Gi
              limits:
                memory: 5Gi
            instances: 1
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: ceph-block
          reclaimPolicy: Delete
          parameters:
            region: us-east-1
