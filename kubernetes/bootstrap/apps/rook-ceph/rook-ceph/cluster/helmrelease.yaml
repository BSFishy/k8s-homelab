---
# yaml-language-server: $schema=https://k8s-schemas.bjw-s.dev/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: rook-ceph-cluster
spec:
  interval: 10m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: v1.18.1
  url: oci://ghcr.io/rook/rook-ceph-cluster
---
# yaml-language-server: $schema=https://k8s-schemas.bjw-s.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
    - name: traefik
      namespace: infra
    - name: snapshot-controller
      namespace: system-controllers
  values:
    monitoring:
      enabled: true
      createPrometheusRules: true

    toolbox:
      enabled: true

    cephClusterSpec:
      cephConfig:
        global:
          bdev_enable_discard: "true" # quote
          bdev_async_discard_threads: "1" # quote
          osd_class_update_on_start: "false" # quote
          device_failure_prediction_mode: local # requires mgr module
        mgr:
          mgr/crash/warn_recent_interval: "7200" # 2h

      cleanupPolicy:
        wipeDevicesFromOtherClusters: true

      crashCollector:
        disable: false

      dashboard:
        enabled: true
        urlPrefix: /
        ssl: true
        prometheusEndpoint: http://prometheus-operated.observability.svc.cluster.local:9090

      mgr:
        modules:
          - name: diskprediction_local
            enabled: true
          - name: insights
            enabled: true
          - name: pg_autoscaler
            enabled: true
          - name: rook
            enabled: true

      network:
        provider: host
        connections:
          requireMsgr2: true

      storage:
        useAllNodes: true
        useAllDevices: false
        devices:
          # gaia-01
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y716078M

          # gaia-02
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y708885P

          # gaia-03
          - name: /dev/disk/by-id/nvme-Samsung_SSD_990_EVO_Plus_2TB_S7U6NU0Y707633R

        config:
          osdsPerDevice: "1"

    cephBlockPools:
      # Critical: size=3, retain, snapshot/backup targeted by Velero
      - name: pool-critical
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-critical
          isDefault: false
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # label SC so Velero schedules can target it
          annotations:
            storage-tier: "critical"

      # High priority: size=3, normal reclaim
      - name: pool-high
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-high
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "high"

      # Regular: size=2 or 3 (pick), baseline
      - name: pool-regular
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-regular
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "regular"

      # Low: size=2, cheaper
      - name: pool-low
        spec:
          failureDomain: host
          replicated:
            size: 2
        storageClass:
          enabled: true
          name: ceph-low
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "low"

      # Extra-low: “no snapshots/backups” (policy-level), minimal features
      - name: pool-extra-low
        spec:
          failureDomain: host
          replicated:
            size: 2
        storageClass:
          enabled: true
          name: ceph-extra-low
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: [discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/fstype: ext4
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "extra-low"
            # Optional: an annotation your Velero schedule uses to EXCLUDE this class
            backup.velero.io/exclude: "true"

      - name: pool-media-nvme
        spec:
          failureDomain: host
          crushRoot: default
          deviceClass: nvme
          replicated:
            size: 2
          parameters:
            compression_mode: none
          enableRBDStats: true
        storageClass:
          enabled: true
          name: ceph-media-nvme
          isDefault: false
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          # NVMe: keep TRIM on, cut atime churn
          mountOptions: [noatime, nodiratime, discard]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            pool: pool-media-nvme-meta
            dataPool: pool-media-nvme-ec
            csi.storage.k8s.io/fstype: xfs
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "bulk-media-nvme"

      # ---------- HDD tier (future) ----------
      - name: pool-media-hdd-meta
        spec:
          failureDomain: host
          crushRoot: default
          deviceClass: hdd
          replicated:
            size: 3
          parameters:
            compression_mode: none
          enableRBDStats: true
        storageClass:
          enabled: false

      - name: pool-media-hdd-ec
        spec:
          failureDomain: host
          crushRoot: default
          deviceClass: hdd
          erasureCoded:
            dataChunks: 6 # HDDs: 6+3 (or 7+2) often balances capacity vs. resiliency
            codingChunks: 3
          parameters:
            compression_mode: none
          enableRBDStats: true
        storageClass:
          enabled: true
          name: ceph-media-hdd
          isDefault: false
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          # HDD: skip discard; still cut atime churn
          mountOptions: [noatime, nodiratime]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            pool: pool-media-hdd-meta
            dataPool: pool-media-hdd-ec
            csi.storage.k8s.io/fstype: xfs
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          annotations:
            storage-tier: "bulk-media-hdd"

    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete
      labels:
        velero.io/csi-volumesnapshot-class: "true"

    cephFileSystems: []
    cephObjectStores: []
